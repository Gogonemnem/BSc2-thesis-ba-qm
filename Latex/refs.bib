
@misc{noauthor_spacy_nodate,
	title = {{spaCy} · {Industrial}-strength {Natural} {Language} {Processing} in {Python}},
	url = {https://spacy.io/},
	abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
	language = {en},
	urldate = {2022-05-16},
}

@article{Blei2003LDA,
	title = {Latent dirichlet allocation},
	volume = {3},
	issn = {1532-4435},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	journal = {The Journal of Machine Learning Research},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	month = mar,
	year = {2003},
	pages = {993--1022},
}

@inproceedings{Liao2019LCC+GBC,
	address = {Hong Kong, China},
	title = {Coupling {Global} and {Local} {Context} for {Unsupervised} {Aspect} {Extraction}},
	abstract = {Aspect words, indicating opinion targets, are essential in expressing and understanding human opinions. To identify aspects, most previous efforts focus on using sequence tagging models trained on human-annotated data. This work studies unsupervised aspect extraction and explores how words appear in global context (on sentence level) and local context (conveyed by neighboring words). We propose a novel neural model, capable of coupling global and local representation to discover aspect words. Experimental results on two benchmarks, laptop and restaurant reviews, show that our model significantly outperforms the state-of-the-art models from previous studies evaluated with varying metrics. Analysis on model output show our ability to learn meaningful and coherent aspect representations. We further investigate how words distribute in global and local context, and find that aspect and non-aspect words do exhibit different context, interpreting our superiority in unsupervised aspect extraction.},
	urldate = {2022-05-11},
	booktitle = {2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Liao, Ming and Li, Jing and Zhang, Haisong and Wang, Lingzhi and Wu, Xixin and Wong, Kam-Fai},
	month = nov,
	year = {2019},
	pages = {4579--4589},
}

@inproceedings{Ramachandran2017UnsupervisedSeq2Seq,
	address = {Copenhagen, Denmark},
	title = {Unsupervised {Pretraining} for {Sequence} to {Sequence} {Learning}},
	abstract = {This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT English→German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from th previous best models on both WMT'14 and WMT'15 English→German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.},
	urldate = {2022-05-13},
	booktitle = {2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2017)},
	publisher = {Association for Computational Linguistics},
	author = {Ramachandran, Prajit and Liu, Peter and Le, Quoc},
	month = sep,
	year = {2017},
	pages = {383--391},
}

@inproceedings{Wang2015Boltzmann,
	address = {Beijing, China},
	title = {Sentiment-{Aspect} {Extraction} based on {Restricted} {Boltzmann} {Machines}},
	urldate = {2022-05-12},
	booktitle = {53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({ACL}-{IJCNLP} 2015)},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Linlin and Liu, Kang and Cao, Zhu and Zhao, Jun and de Melo, Gerard},
	month = jul,
	year = {2015},
	pages = {616--625},
}

@inproceedings{Xu2012JAS,
	address = {New York, NY, USA},
	series = {{CIKM} '12},
	title = {Towards jointly extracting aspects and aspect-specific sentiment knowledge},
	isbn = {9781450311564},
	abstract = {In this paper, we aim to jointly extract aspects and aspect-specific sentiment knowledge from online reviews, where the sentiment knowledge refers to the aspect-specific opinion words along with their aspect-aware sentiment polarities. To this end, we propose a Joint Aspect/Sentiment model (JAS). JAS detects aspect-specific opinion words by integrating opinion word lexicon knowledge to explicitly separate opinion words from factual words. More importantly, JAS exploits sentiment prior and aspect-contextual sentence-level co-occurrences of opinion words in reviews to further identify aspect-aware sentiment polarities for the opinion words. We apply the learned aspect-specific sentiment knowledge to practical aspect-level sentiment analysis tasks. Experimental results show the effectiveness of JAS in learning aspect-specific sentiment knowledge and the practical value of this knowledge when applied to aspect-level sentiment classification.},
	urldate = {2022-05-11},
	booktitle = {21st {ACM} international conference on {Information} and knowledge management ({CIKM} 2012)},
	publisher = {Association for Computing Machinery},
	author = {Xu, Xueke and Tan, Songbo and Liu, Yue and Cheng, Xueqi and Lin, Zheng},
	month = oct,
	year = {2012},
	keywords = {aspect-level sentiment analysis, aspect-specific sentiment knowledge, joint aspect/setniment model, online reviews},
	pages = {1895--1899},
}

@inproceedings{Zhao2010MaxEnt-LDA,
	address = {Cambridge, MA},
	title = {Jointly {Modeling} {Aspects} and {Opinions} with a {MaxEnt}-{LDA} {Hybrid}},
	urldate = {2022-05-12},
	booktitle = {2010 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2010)},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Xin and Jiang, Jing and Yan, Hongfei and Li, Xiaoming},
	month = oct,
	year = {2010},
	pages = {56--65},
}

@incollection{Zhuang2020JASA,
	address = {New York, NY, USA},
	title = {Joint {Aspect}-{Sentiment} {Analysis} with {Minimal} {User} {Guidance}},
	isbn = {9781450380164},
	abstract = {Aspect-based sentiment analysis is a substantial step towards text understanding which benefits numerous applications. Since most existing algorithms require a large amount of labeled data or substantial external language resources, applying them on a new domain or a new language is usually expensive and time-consuming. We aim to build an aspect-based sentiment analysis model from an unlabeled corpus with minimal guidance from users, i.e., only a small set of seed words for each aspect class and each sentiment class. We employ an autoencoder structure with attention to learn two dictionary matrices for aspect and sentiment respectively where each row of the dictionary serves as an embedding vector for an aspect or a sentiment class. We propose to utilize the user-given seed words to regularize the dictionary learning. In addition, we improve the model by joining the aspect and sentiment encoder in the reconstruction of sentiment in sentences. The joint structure enables sentiment embeddings in the dictionary to be tuned towards the aspect-specific sentiment words for each aspect, which benefits the classification performance. We conduct experiments on two real data sets to verify the effectiveness of our models.},
	urldate = {2022-05-11},
	booktitle = {43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval} ({SIGIR} 2020)},
	publisher = {Association for Computing Machinery},
	author = {Zhuang, Honglei and Guo, Fang and Zhang, Chao and Liu, Liyuan and Han, Jiawei},
	month = jul,
	year = {2020},
	keywords = {aspect-based sentiment analysis, autoencoder, weakly-supervised},
	pages = {1241--1250},
}

@inproceedings{He2017ABAE,
	address = {Vancouver, Canada},
	title = {An {Unsupervised} {Neural} {Attention} {Model} for {Aspect} {Extraction}},
	abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
	urldate = {2022-05-09},
	booktitle = {55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({ACL} 2017)},
	publisher = {Association for Computational Linguistics},
	author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
	month = jul,
	year = {2017},
	pages = {388--397},
}

@article{Zhang2022Survey,
	title = {A {Survey} on {Aspect}-{Based} {Sentiment} {Analysis}: {Tasks}, {Methods}, and {Challenges}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {A {Survey} on {Aspect}-{Based} {Sentiment} {Analysis}},
	abstract = {As an important fine-grained sentiment analysis problem, aspect-based sentiment analysis (ABSA), aiming to analyze and understand people's opinions at the aspect level, has been attracting considerable interest in the last decade. To handle ABSA in different scenarios, various tasks have been introduced for analyzing different sentiment elements and their relations, including the aspect term, aspect category, opinion term, and sentiment polarity. Unlike early ABSA works focusing on a single sentiment element, many compound ABSA tasks involving multiple elements have been studied in recent years for capturing more complete aspect-level sentiment information. However, a systematic review of various ABSA tasks and their corresponding solutions is still lacking, which we aim to fill in this survey. More specifically, we provide a new taxonomy for ABSA which organizes existing studies from the axes of concerned sentiment elements, with an emphasis on recent advances of compound ABSA tasks. From the perspective of solutions, we summarize the utilization of pre-trained language models for ABSA, which improved the performance of ABSA to a new stage. Besides, techniques for building more practical ABSA systems in cross-domain/lingual scenarios are discussed. Finally, we review some emerging topics and discuss some open challenges to outlook potential future directions of ABSA.},
	urldate = {2022-05-09},
	author = {Zhang, Wenxuan and Li, Xin and Deng, Yang and Bing, Lidong and Lam, Wai},
	year = {2022},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{Tulkens2020CAt,
	title = {Embarrassingly {Simple} {Unsupervised} {Aspect} {Extraction}},
	abstract = {We present a simple but effective method for aspect identification in sentiment analysis. Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages. We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable. Previous work relied on syntactic features and complex neural models. We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed. The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat},
	urldate = {2022-05-09},
	journal = {arXiv:2004.13580 [cs]},
	author = {Tulkens, Stéphan and van Cranenburgh, Andreas},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@article{Meskele2020ALDONAr,
	title = {{ALDONAr}: {A} hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalized domain ontology and a regularized neural attention model},
	volume = {57},
	issn = {03064573},
	shorttitle = {{ALDONAr}},
	language = {en},
	number = {3},
	urldate = {2022-05-13},
	journal = {Information Processing \& Management},
	author = {Meškelė, Donatas and Frasincar, Flavius},
	month = may,
	year = {2020},
	pages = {102211},
}

@article{Liu2012SAOP,
	title = {Sentiment {Analysis} and {Opinion} {Mining}},
	volume = {5},
	issn = {1947-4040},
	number = {1},
	urldate = {2022-05-09},
	journal = {Synthesis Lectures on Human Language Technologies},
	author = {Liu, Bing},
	month = may,
	year = {2012},
	pages = {1--167},
}

@article{Kumar2021CASC,
	title = {{BERT} {Based} {Semi}-{Supervised} {Hybrid} {Approach} for {Aspect} and {Sentiment} {Classification}},
	volume = {53},
	issn = {1573-773X},
	abstract = {Aspect-based sentiment analysis (ABSA) includes two sub-tasks namely, aspect extraction and aspect-level sentiment classification. Most existing works address these sub-tasks independently by applying a supervised learning approach using labeled data. However, obtaining such labeled sentences is difficult and extremely expensive. Hence, it is important to solve ABSA without taking a dependency on labeled sentences. In this work, we propose a three-step semi-supervised hybrid approach that jointly detects an aspect and its associated sentiment in a given review sentence. The first step of our approach takes a small set of seed words for each aspect and sentiment class to construct respective semantically coherent class vocabularies. The second step makes use of these constructed vocabularies along with POS tags to label a subset of sentences from the training corpus. As we adopt a semi-automated approach to label the data, this process may induce noise in the labels during the annotation. In the last step, we use such labeled sentences to build a noise-robust deep neural network for aspect and sentiment classification. We conduct experiments on two real data sets to verify the effectiveness of our model (https://github.com/Raghu150999/UnsupervisedABSA).},
	language = {en},
	number = {6},
	urldate = {2022-05-09},
	journal = {Neural Processing Letters},
	author = {Kumar, Avinash and Gupta, Pranjal and Balan, Raghunathan and Neti, Lalita Bhanu Murthy and Malapati, Aruna},
	month = dec,
	year = {2021},
	pages = {4207--4224},
}

@article{Karamanolakis2019Seed,
	title = {Leveraging {Just} a {Few} {Keywords} for {Fine}-{Grained} {Aspect} {Detection} {Through} {Weakly} {Supervised} {Co}-{Training}},
	abstract = {User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., price, quality, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, unsupervised topic models often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches do not effectively leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher); in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous weakly supervised approaches (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.},
	urldate = {2022-05-09},
	journal = {arXiv:1909.00415 [cs, stat]},
	author = {Karamanolakis, Giannis and Hsu, Daniel and Gravano, Luis},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{Fazly2009UnsupervisedTokenClass,
	title = {Unsupervised {Type} and {Token} {Identification} of {Idiomatic} {Expressions}},
	volume = {35},
	issn = {0891-2017, 1530-9312},
	abstract = {Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them. They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics. In this article, we look into the usefulness of some of the identified linguistic properties of idioms for their automatic recognition. Specifically, we develop statistical measures that each model a specific property of idiomatic expressions by looking at their actual usage patterns in text. We use these statistical measures in a type-based classification task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of the measures in a token identification task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context.},
	language = {en},
	number = {1},
	urldate = {2022-05-13},
	journal = {Computational Linguistics},
	author = {Fazly, Afsaneh and Cook, Paul and Stevenson, Suzanne},
	month = mar,
	year = {2009},
	pages = {61--103},
}

@article{Cui2020UnsupervisedMRC,
	title = {Unsupervised {Explanation} {Generation} for {Machine} {Reading} {Comprehension}},
	abstract = {With the blooming of various Pre-trained Language Models (PLMs), Machine Reading Comprehension (MRC) has embraced significant improvements on various benchmarks and even surpass human performances. However, the existing works only target on the accuracy of the final predictions and neglect the importance of the explanations for the prediction, which is a big obstacle when utilizing these models in real-life applications to convince humans. In this paper, we propose a self-explainable framework for the machine reading comprehension task. The main idea is that the proposed system tries to use less passage information and achieve similar results compared to the system that uses the whole passage, while the filtered passage will be used as explanations. We carried out experiments on three multiple-choice MRC datasets, and found that the proposed system could achieve consistent improvements over baseline systems. To evaluate the explainability, we compared our approach with the traditional attention mechanism in human evaluations and found that the proposed system has a notable advantage over the latter one.},
	urldate = {2022-05-13},
	journal = {arXiv:2011.06737 [cs]},
	author = {Cui, Yiming and Liu, Ting and Wang, Shijin and Hu, Guoping},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{Huang2020JASen,
	address = {Online},
	title = {Weakly-{Supervised} {Aspect}-{Based} {Sentiment} {Analysis} via {Joint} {Aspect}-{Sentiment} {Topic} {Embedding}},
	abstract = {Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts. We propose to first learn {\textbackslash}textlesssentiment, aspect{\textbackslash}textgreater joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4\% and 5.1\% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.},
	urldate = {2022-05-11},
	booktitle = {2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Jiaxin and Meng, Yu and Guo, Fang and Ji, Heng and Han, Jiawei},
	month = nov,
	year = {2020},
	pages = {6989--6999},
}

@inproceedings{Wallaart2019HAABSA,
	title = {A {Hybrid} {Approach} for {Aspect}-{Based} {Sentiment} {Analysis} {Using} a {Lexicalized} {Domain} {Ontology} and {Attentional} {Neural} {Models}},
	volume = {11503},
	isbn = {9783030213473},
	language = {en},
	urldate = {2022-03-06},
	booktitle = {16th {Extended} {Semantic} {Web} {Conference} ({ESWB} 2019)},
	publisher = {Springer},
	author = {Wallaart, Olaf and Frasincar, Flavius},
	year = {2019},
	pages = {363--378},
}

@article{Honnibal2020Spacy,
	title = {{spaCy}: {Industrial}-strength natural language processing in python},
	abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
	language = {en},
	journal = {Zenodo, Honolulu, HI, USA},
	author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
	year = {2020},
}

@inproceedings{Tang2016Yelp,
	title = {Aspect {Level} {Sentiment} {Classification} with {Deep} {Memory} {Network}},
	abstract = {We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.},
	urldate = {2022-03-10},
	booktitle = {2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2016)},
	publisher = {ACL},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	year = {2016},
	keywords = {Computer Science - Computation and Language},
	pages = {214--222},
}

@inproceedings{Trusca2020HAABSA++,
	series = {{LNCS}},
	title = {A {Hybrid} {Approach} for {Aspect}-{Based} {Sentiment} {Analysis} {Using} {Deep} {Contextual} {Word} {Embeddings} and {Hierarchical} {Attention}},
	volume = {12128},
	abstract = {The Web has become the main platform where people express their opinions about entities of interest and their associated aspects. Aspect-Based Sentiment Analysis (ABSA) aims to automatically compute the sentiment towards these aspects from opinionated text. In this paper we extend the state-of-the-art Hybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) method in two directions. First we replace the non-contextual word embeddings with deep contextual word embeddings in order to better cope with the word semantics in a given text. Second, we use hierarchical attention by adding an extra attention layer to the HAABSA high-level representations in order to increase the method flexibility in modeling the input data. Using two standard datasets (SemEval 2015 and SemEval 2016) we show that the proposed extensions improve the accuracy of the built model for ABSA.},
	language = {en},
	booktitle = {20th {International} {Conference} on {Web} {Engineering} ({ICWE} 2020)},
	publisher = {Springer},
	author = {Truşcǎ, Maria Mihaela and Wassenberg, Daan and Frasincar, Flavius and Dekker, Rommert},
	year = {2020},
	pages = {365--380},
}

@inproceedings{Schouten2017Ontology,
	series = {{LNCS}},
	title = {Ontology-{Enhanced} {Aspect}-{Based} {Sentiment} {Analysis}},
	volume = {10360},
	abstract = {With many people freely expressing their opinions and feelings on the Web, much research has gone into modeling and monetizing opinionated, and usually unstructured and textual, Web-based content. Aspect-based sentiment analysis aims to extract the fine-grained topics, or aspects, that people are talking about, together with the sentiment expressed on those aspects. This allows for a detailed analysis of the sentiment expressed in, for instance, product and service reviews. In this work we focus on knowledge-driven solutions that aim to complement standard machine learning methods. By encoding common domain knowledge into a knowledge repository, or ontology, we are able to exploit this information to improve classification performance for both aspect detection and aspect sentiment analysis. For aspect detection, the ontology-enhanced method needs only 20\% of the training data to achieve results comparable with a standard bag-of-words approach that uses all training data.},
	language = {en},
	booktitle = {17th {International} {Conference} on {Web} {Engineering} ({ICWE} 2017)},
	publisher = {Springer},
	author = {Schouten, Kim and Frasincar, Flavius and de Jong, Franciska},
	year = {2017},
	pages = {302--320},
}

@inproceedings{Pontiki2016SemEval,
	title = {{SemEval}-2016 {Task} 5: {Aspect} {Based} {Sentiment} {Analysis}},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {10th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2016)},
	publisher = {ACL},
	author = {Pontiki, Maria and Galanis, Dimitris and Papageorgiou, Haris and Androutsopoulos, Ion and Manandhar, Suresh and AL-Smadi, Mohammad and Al-Ayyoub, Mahmoud and Zhao, Yanyan and Qin, Bing and De Clercq, Orphee and Hoste, Veronique and Apidianaki, Marianna and Tannier, Xavier and Loukachevitch, Natalia and Kotelnikov, Evgeniy and Bel, Núria and Jiménez-Zafra, Salud María and Eryiğit, Gülşen},
	year = {2016},
	pages = {19--30},
}

@inproceedings{Pontiki2015SemEval,
	title = {{SemEval}-2015 {Task} 12: {Aspect} {Based} {Sentiment} {Analysis}},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {9th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2015)},
	publisher = {ACL},
	author = {Pontiki, Maria and Galanis, Dimitris and Papageorgiou, Haris and Manandhar, Suresh and Androutsopoulos, Ion},
	year = {2015},
	pages = {486--495},
}

@inproceedings{Hu2004Rules,
	title = {Mining and {Summarizing} {Customer} {Reviews}},
	language = {en},
	urldate = {2022-03-06},
	booktitle = {10th {ACM} {SIGKDD} international conference on {Knowledge} discovery and {Data} {Mining} ({KDD} 2004)},
	publisher = {ACM},
	author = {Hu, Minqing and Liu, Bing},
	year = {2004},
	pages = {168--177},
}

@article{Zheng2018LCR-Rot,
	title = {Left-{Center}-{Right} {Separated} {Neural} {Network} for {Aspect}-based {Sentiment} {Analysis} with {Rotatory} {Attention}},
	abstract = {Deep learning techniques have achieved success in aspect-based sentiment analysis in recent years. However, there are two important issues that still remain to be further studied, i.e., 1) how to efficiently represent the target especially when the target contains multiple words; 2) how to utilize the interaction between target and left/right contexts to capture the most important words in them. In this paper, we propose an approach, called left-center-right separated neural network with rotatory attention (LCR-Rot), to better address the two problems. Our approach has two characteristics: 1) it has three separated LSTMs, i.e., left, center and right LSTMs, corresponding to three parts of a review (left context, target phrase and right context); 2) it has a rotatory attention mechanism which models the relation between target and left/right contexts. The target2context attention is used to capture the most indicative sentiment words in left/right contexts. Subsequently, the context2target attention is used to capture the most important word in the target. This leads to a two-side representation of the target: left-aware target and right-aware target. We compare our approach on three benchmark datasets with ten related methods proposed recently. The results show that our approach significantly outperforms the state-of-the-art techniques.},
	urldate = {2022-03-06},
	journal = {arXiv preprint arXiv:1802.00892},
	author = {Zheng, Shiliang and Xia, Rui},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{Devlin2019BERT,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-03-09},
	booktitle = {17th {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({NAACL}-{HLT} 2019)},
	publisher = {ACL},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	keywords = {Computer Science - Computation and Language},
	pages = {4171--4186},
}

@article{Schouten2016Survey,
	title = {Survey on {Aspect}-{Level} {Sentiment} {Analysis}},
	volume = {28},
	number = {3},
	urldate = {2022-03-06},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Schouten, Kim and Frasincar, Flavius},
	year = {2016},
	pages = {813--830},
}
