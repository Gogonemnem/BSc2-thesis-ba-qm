
@article{Kingma2017Adam,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2022-06-23},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	keywords = {Computer Science - Machine Learning},
}

@article{Li2018Hyperband,
	title = {Hyperband: {A} {Novel} {Bandit}-{Based} {Approach} to {Hyperparameter} {Optimization}},
	volume = {18},
	issn = {1533-7928},
	shorttitle = {Hyperband},
	url = {http://jmlr.org/papers/v18/16-558.html},
	abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Ã¸uralg , for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Ã¸uralg with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Ã¸uralg can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
	number = {185},
	urldate = {2022-06-23},
	journal = {Journal of Machine Learning Research},
	author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2018},
	pages = {1--52},
}

@inproceedings{Xu2019DKBERT,
	address = {Minneapolis, Minnesota},
	title = {{BERT} {Post}-{Training} for {Review} {Reading} {Comprehension} and {Aspect}-based {Sentiment} {Analysis}},
	url = {https://aclanthology.org/N19-1242},
	doi = {10.18653/v1/N19-1242},
	abstract = {Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions. We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective.},
	urldate = {2022-06-23},
	booktitle = {2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Hu and Liu, Bing and Shu, Lei and Yu, Philip},
	month = jun,
	year = {2019},
	pages = {2324--2335},
}

@inproceedings{Trusca2020HAABSA++,
	address = {Cham},
	title = {A {Hybrid} {Approach} for {Aspect}-{Based} {Sentiment} {Analysis} {Using} {Deep} {Contextual} {Word} {Embeddings} and {Hierarchical} {Attention}},
	isbn = {9783030505783},
	doi = {10.1007/978-3-030-50578-3_25},
	abstract = {The Web has become the main platform where people express their opinions about entities of interest and their associated aspects. Aspect-Based Sentiment Analysis (ABSA) aims to automatically compute the sentiment towards these aspects from opinionated text. In this paper we extend the state-of-the-art Hybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) method in two directions. First we replace the non-contextual word embeddings with deep contextual word embeddings in order to better cope with the word semantics in a given text. Second, we use hierarchical attention by adding an extra attention layer to the HAABSA high-level representations in order to increase the method flexibility in modeling the input data. Using two standard datasets (SemEval 2015 and SemEval 2016) we show that the proposed extensions improve the accuracy of the built model for ABSA.},
	language = {en},
	booktitle = {20th {International} {Conference} on {Web} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Truşcǎ, Maria Mihaela and Wassenberg, Daan and Frasincar, Flavius and Dekker, Rommert},
	editor = {Bielikova, Maria and Mikkonen, Tommi and Pautasso, Cesare},
	year = {2020},
	pages = {365--380},
}

@inproceedings{Liao2019LCC+GBC,
	address = {Hong Kong, China},
	title = {Coupling {Global} and {Local} {Context} for {Unsupervised} {Aspect} {Extraction}},
	url = {https://aclanthology.org/D19-1465},
	doi = {10.18653/v1/D19-1465},
	abstract = {Aspect words, indicating opinion targets, are essential in expressing and understanding human opinions. To identify aspects, most previous efforts focus on using sequence tagging models trained on human-annotated data. This work studies unsupervised aspect extraction and explores how words appear in global context (on sentence level) and local context (conveyed by neighboring words). We propose a novel neural model, capable of coupling global and local representation to discover aspect words. Experimental results on two benchmarks, laptop and restaurant reviews, show that our model significantly outperforms the state-of-the-art models from previous studies evaluated with varying metrics. Analysis on model output show our ability to learn meaningful and coherent aspect representations. We further investigate how words distribute in global and local context, and find that aspect and non-aspect words do exhibit different context, interpreting our superiority in unsupervised aspect extraction.},
	urldate = {2022-05-23},
	booktitle = {2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {ACL},
	author = {Liao, Ming and Li, Jing and Zhang, Haisong and Wang, Lingzhi and Wu, Xixin and Wong, Kam-Fai},
	month = nov,
	year = {2019},
	pages = {4579--4589},
}

@inproceedings{Wallaart2019HAABSA,
	address = {Cham},
	title = {A {Hybrid} {Approach} for {Aspect}-{Based} {Sentiment} {Analysis} {Using} a {Lexicalized} {Domain} {Ontology} and {Attentional} {Neural} {Models}},
	isbn = {9783030213480},
	doi = {10.1007/978-3-030-21348-0\_24},
	abstract = {This work focuses on sentence-level aspect-based sentiment analysis for restaurant reviews. A two-stage sentiment analysis algorithm is proposed. In this method, first a lexicalized domain ontology is used to predict the sentiment and as a back-up algorithm a neural network with a rotatory attention mechanism (LCR-Rot) is utilized. Furthermore, two features are added to the backup algorithm. The first extension changes the order in which the rotatory attention mechanism operates (LCR-Rot-inv). The second extension runs over the rotatory attention mechanism for multiple iterations (LCR-Rot-hop). Using the SemEval-2015 and SemEval-2016 data, we conclude that the two-stage method outperforms the baseline methods, albeit with a small percentage. Moreover, we find that the method where we iterate multiple times over a rotatory attention mechanism has the best performance.},
	language = {en},
	booktitle = {16th {Extended} {Semantic} {Web} {Conference}},
	publisher = {Springer International Publishing},
	author = {Wallaart, Olaf and Frasincar, Flavius},
	editor = {Hitzler, Pascal and Fernández, Miriam and Janowicz, Krzysztof and Zaveri, Amrapali and Gray, Alasdair J.G. and Lopez, Vanessa and Haller, Armin and Hammar, Karl},
	year = {2019},
	pages = {363--378},
}

@inproceedings{Wang2015Boltzmann,
	address = {Beijing, China},
	title = {Sentiment-{Aspect} {Extraction} based on {Restricted} {Boltzmann} {Machines}},
	url = {https://aclanthology.org/P15-1060},
	doi = {10.3115/v1/P15-1060},
	urldate = {2022-05-23},
	booktitle = {53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {ACL},
	author = {Wang, Linlin and Liu, Kang and Cao, Zhu and Zhao, Jun and de Melo, Gerard},
	month = jul,
	year = {2015},
	pages = {616--625},
}

@incollection{Zhuang2020JASA,
	address = {New York, NY, USA},
	title = {Joint {Aspect}-{Sentiment} {Analysis} with {Minimal} {User} {Guidance}},
	isbn = {9781450380164},
	url = {https://doi.org/10.1145/3397271.3401179},
	abstract = {Aspect-based sentiment analysis is a substantial step towards text understanding which benefits numerous applications. Since most existing algorithms require a large amount of labeled data or substantial external language resources, applying them on a new domain or a new language is usually expensive and time-consuming. We aim to build an aspect-based sentiment analysis model from an unlabeled corpus with minimal guidance from users, i.e., only a small set of seed words for each aspect class and each sentiment class. We employ an autoencoder structure with attention to learn two dictionary matrices for aspect and sentiment respectively where each row of the dictionary serves as an embedding vector for an aspect or a sentiment class. We propose to utilize the user-given seed words to regularize the dictionary learning. In addition, we improve the model by joining the aspect and sentiment encoder in the reconstruction of sentiment in sentences. The joint structure enables sentiment embeddings in the dictionary to be tuned towards the aspect-specific sentiment words for each aspect, which benefits the classification performance. We conduct experiments on two real data sets to verify the effectiveness of our models.},
	urldate = {2022-05-22},
	booktitle = {43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Zhuang, Honglei and Guo, Fang and Zhang, Chao and Liu, Liyuan and Han, Jiawei},
	month = jul,
	year = {2020},
	keywords = {aspect-based sentiment analysis, autoencoder, weakly-supervised},
	pages = {1241--1250},
}

@inproceedings{Zhao2010MaxEnt-LDA,
	address = {Cambridge, MA},
	title = {Jointly {Modeling} {Aspects} and {Opinions} with a {MaxEnt}-{LDA} {Hybrid}},
	url = {https://aclanthology.org/D10-1006},
	urldate = {2022-05-23},
	booktitle = {2010 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {ACL},
	author = {Zhao, Xin and Jiang, Jing and Yan, Hongfei and Li, Xiaoming},
	month = oct,
	year = {2010},
	pages = {56--65},
}

@inproceedings{Xu2012JAS,
	address = {New York, NY, USA},
	series = {{CIKM} '12},
	title = {Towards jointly extracting aspects and aspect-specific sentiment knowledge},
	isbn = {9781450311564},
	url = {https://doi.org/10.1145/2396761.2398539},
	doi = {10.1145/2396761.2398539},
	abstract = {In this paper, we aim to jointly extract aspects and aspect-specific sentiment knowledge from online reviews, where the sentiment knowledge refers to the aspect-specific opinion words along with their aspect-aware sentiment polarities. To this end, we propose a Joint Aspect/Sentiment model (JAS). JAS detects aspect-specific opinion words by integrating opinion word lexicon knowledge to explicitly separate opinion words from factual words. More importantly, JAS exploits sentiment prior and aspect-contextual sentence-level co-occurrences of opinion words in reviews to further identify aspect-aware sentiment polarities for the opinion words. We apply the learned aspect-specific sentiment knowledge to practical aspect-level sentiment analysis tasks. Experimental results show the effectiveness of JAS in learning aspect-specific sentiment knowledge and the practical value of this knowledge when applied to aspect-level sentiment classification.},
	urldate = {2022-05-22},
	booktitle = {21st {ACM} international conference on {Information} and knowledge management},
	publisher = {ACM},
	author = {Xu, Xueke and Tan, Songbo and Liu, Yue and Cheng, Xueqi and Lin, Zheng},
	month = oct,
	year = {2012},
	keywords = {aspect-level sentiment analysis, aspect-specific sentiment knowledge, joint aspect/setniment model, online reviews},
	pages = {1895--1899},
}

@inproceedings{Tulkens2020CAt,
	address = {Online},
	title = {Embarrassingly {Simple} {Unsupervised} {Aspect} {Extraction}},
	url = {https://aclanthology.org/2020.acl-main.290},
	doi = {10.18653/v1/2020.acl-main.290},
	abstract = {We present a simple but effective method for aspect identification in sentiment analysis. Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages. We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable. Previous work relied on syntactic features and complex neural models. We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed. The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.},
	urldate = {2022-05-23},
	booktitle = {58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {ACL},
	author = {Tulkens, Stéphan and van Cranenburgh, Andreas},
	month = jul,
	year = {2020},
	pages = {3182--3187},
}

@inproceedings{Tang2016Yelp,
	address = {Austin, Texas},
	title = {Aspect {Level} {Sentiment} {Classification} with {Deep} {Memory} {Network}},
	url = {https://aclanthology.org/D16-1021},
	doi = {10.18653/v1/D16-1021},
	urldate = {2022-05-23},
	booktitle = {2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {ACL},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	month = nov,
	year = {2016},
	pages = {214--224},
}

@inproceedings{Schouten2017Ontology,
	address = {Cham},
	series = {{LCNS}},
	title = {Ontology-{Enhanced} {Aspect}-{Based} {Sentiment} {Analysis}},
	isbn = {9783319601311},
	doi = {10.1007/978-3-319-60131-1_17},
	abstract = {With many people freely expressing their opinions and feelings on the Web, much research has gone into modeling and monetizing opinionated, and usually unstructured and textual, Web-based content. Aspect-based sentiment analysis aims to extract the fine-grained topics, or aspects, that people are talking about, together with the sentiment expressed on those aspects. This allows for a detailed analysis of the sentiment expressed in, for instance, product and service reviews. In this work we focus on knowledge-driven solutions that aim to complement standard machine learning methods. By encoding common domain knowledge into a knowledge repository, or ontology, we are able to exploit this information to improve classification performance for both aspect detection and aspect sentiment analysis. For aspect detection, the ontology-enhanced method needs only 20\% of the training data to achieve results comparable with a standard bag-of-words approach that uses all training data.},
	language = {en},
	booktitle = {Web {Engineering}},
	publisher = {Springer International Publishing},
	author = {Schouten, Kim and Frasincar, Flavius and de Jong, Franciska},
	editor = {Cabot, Jordi and De Virgilio, Roberto and Torlone, Riccardo},
	year = {2017},
	pages = {302--320},
}

@inproceedings{Ramachandran2017UnsupervisedSeq2Seq,
	address = {Copenhagen, Denmark},
	title = {Unsupervised {Pretraining} for {Sequence} to {Sequence} {Learning}},
	url = {https://aclanthology.org/D17-1039},
	doi = {10.18653/v1/D17-1039},
	abstract = {This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT English→German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from th previous best models on both WMT'14 and WMT'15 English→German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.},
	urldate = {2022-05-23},
	booktitle = {2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {ACL},
	author = {Ramachandran, Prajit and Liu, Peter and Le, Quoc},
	month = sep,
	year = {2017},
	pages = {383--391},
}

@inproceedings{Pontiki2016SemEval,
	address = {San Diego, California},
	title = {{SemEval}-2016 {Task} 5: {Aspect} {Based} {Sentiment} {Analysis}},
	shorttitle = {{SemEval}-2016 {Task} 5},
	url = {https://aclanthology.org/S16-1002},
	doi = {10.18653/v1/S16-1002},
	urldate = {2022-05-23},
	booktitle = {10th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2016)},
	publisher = {ACL},
	author = {Pontiki, Maria and Galanis, Dimitris and Papageorgiou, Haris and Androutsopoulos, Ion and Manandhar, Suresh and AL-Smadi, Mohammad and Al-Ayyoub, Mahmoud and Zhao, Yanyan and Qin, Bing and De Clercq, Orphée and Hoste, Véronique and Apidianaki, Marianna and Tannier, Xavier and Loukachevitch, Natalia and Kotelnikov, Evgeniy and Bel, Nuria and Jiménez-Zafra, Salud María and Eryiğit, Gülşen},
	month = jun,
	year = {2016},
	pages = {19--30},
}

@inproceedings{Pontiki2015SemEval,
	address = {Denver, Colorado},
	title = {{SemEval}-2015 {Task} 12: {Aspect} {Based} {Sentiment} {Analysis}},
	shorttitle = {{SemEval}-2015 {Task} 12},
	url = {https://aclanthology.org/S15-2082},
	doi = {10.18653/v1/S15-2082},
	urldate = {2022-05-23},
	booktitle = {9th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2015)},
	publisher = {ACL},
	author = {Pontiki, Maria and Galanis, Dimitris and Papageorgiou, Haris and Manandhar, Suresh and Androutsopoulos, Ion},
	month = jun,
	year = {2015},
	pages = {486--495},
}

@inproceedings{Karamanolakis2019Seed,
	address = {Hong Kong, China},
	title = {Leveraging {Just} a {Few} {Keywords} for {Fine}-{Grained} {Aspect} {Detection} {Through} {Weakly} {Supervised} {Co}-{Training}},
	url = {https://aclanthology.org/D19-1468},
	doi = {10.18653/v1/D19-1468},
	abstract = {User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., price, quality, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, unsupervised topic models often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches fail to leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher); in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous weakly supervised approaches (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.},
	urldate = {2022-05-23},
	booktitle = {2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {ACL},
	author = {Karamanolakis, Giannis and Hsu, Daniel and Gravano, Luis},
	month = nov,
	year = {2019},
	pages = {4611--4621},
}

@inproceedings{Huang2020JASen,
	address = {Online},
	title = {Weakly-{Supervised} {Aspect}-{Based} {Sentiment} {Analysis} via {Joint} {Aspect}-{Sentiment} {Topic} {Embedding}},
	url = {https://aclanthology.org/2020.emnlp-main.568},
	doi = {10.18653/v1/2020.emnlp-main.568},
	abstract = {Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts. We propose to first learn {\textbackslash}textlesssentiment, aspect{\textbackslash}textgreater joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4\% and 5.1\% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.},
	urldate = {2022-05-23},
	booktitle = {2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {ACL},
	author = {Huang, Jiaxin and Meng, Yu and Guo, Fang and Ji, Heng and Han, Jiawei},
	month = nov,
	year = {2020},
	pages = {6989--6999},
}

@inproceedings{Hu2004Rules,
	address = {New York, NY, USA},
	series = {{KDD} '04},
	title = {Mining and summarizing customer reviews},
	isbn = {9781581138887},
	url = {https://doi.org/10.1145/1014052.1014073},
	doi = {10.1145/1014052.1014073},
	abstract = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
	urldate = {2022-05-22},
	booktitle = {10th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Hu, Minqing and Liu, Bing},
	month = aug,
	year = {2004},
	keywords = {reviews, sentiment classification, summarization, text mining},
	pages = {168--177},
}

@inproceedings{He2017ABAE,
	address = {Vancouver, Canada},
	title = {An {Unsupervised} {Neural} {Attention} {Model} for {Aspect} {Extraction}},
	url = {https://aclanthology.org/P17-1036},
	doi = {10.18653/v1/P17-1036},
	abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
	urldate = {2022-05-23},
	booktitle = {55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {ACL},
	author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
	month = jul,
	year = {2017},
	pages = {388--397},
}

@inproceedings{Devlin2019BERT,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-05-23},
	booktitle = {2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {ACL},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@article{Zheng2018LCR-Rot,
	title = {Left-{Center}-{Right} {Separated} {Neural} {Network} for {Aspect}-based {Sentiment} {Analysis} with {Rotatory} {Attention}},
	url = {http://arxiv.org/abs/1802.00892},
	abstract = {Deep learning techniques have achieved success in aspect-based sentiment analysis in recent years. However, there are two important issues that still remain to be further studied, i.e., 1) how to efficiently represent the target especially when the target contains multiple words; 2) how to utilize the interaction between target and left/right contexts to capture the most important words in them. In this paper, we propose an approach, called left-center-right separated neural network with rotatory attention (LCR-Rot), to better address the two problems. Our approach has two characteristics: 1) it has three separated LSTMs, i.e., left, center and right LSTMs, corresponding to three parts of a review (left context, target phrase and right context); 2) it has a rotatory attention mechanism which models the relation between target and left/right contexts. The target2context attention is used to capture the most indicative sentiment words in left/right contexts. Subsequently, the context2target attention is used to capture the most important word in the target. This leads to a two-side representation of the target: left-aware target and right-aware target. We compare our approach on three benchmark datasets with ten related methods proposed recently. The results show that our approach significantly outperforms the state-of-the-art techniques.},
	urldate = {2022-05-23},
	journal = {arXiv:1802.00892 [cs]},
	author = {Zheng, Shiliang and Xia, Rui},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Computation and Language},
}

@article{Liu2012SAOP,
	title = {Sentiment {Analysis} and {Opinion} {Mining}},
	volume = {5},
	issn = {1947-4040},
	url = {https://www.morganclaypool.com/doi/abs/10.2200/s00416ed1v01y201204hlt016},
	doi = {10.2200/S00416ED1V01Y201204HLT016},
	number = {1},
	urldate = {2022-05-23},
	journal = {Synthesis Lectures on Human Language Technologies},
	author = {Liu, Bing},
	month = may,
	year = {2012},
	pages = {1--167},
}

@article{Schouten2016Survey,
	title = {Survey on {Aspect}-{Level} {Sentiment} {Analysis}},
	volume = {28},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2015.2485209},
	abstract = {The field of sentiment analysis, in which sentiment is gathered, analyzed, and aggregated from text, has seen a lot of attention in the last few years. The corresponding growth of the field has resulted in the emergence of various subareas, each addressing a different level of analysis or research question. This survey focuses on aspect-level sentiment analysis, where the goal is to find and aggregate sentiment on entities mentioned within documents or aspects of them. An in-depth overview of the current state-of-the-art is given, showing the tremendous progress that has already been made in finding both the target, which can be an entity as such, or some aspect of it, and the corresponding sentiment. Aspect-level sentiment analysis yields very fine-grained sentiment information which can be useful for applications in various domains. Current solutions are categorized based on whether they provide a method for aspect detection, sentiment analysis, or both. Furthermore, a breakdown based on the type of algorithm used is provided. For each discussed study, the reported performance is included. To facilitate the quantitative evaluation of the various proposed methods, a call is made for the standardization of the evaluation methodology that includes the use of shared data sets. Semanticallyrich concept-centric aspect-level sentiment analysis is discussed and identified as one of the most promising future research direction.},
	number = {3},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Schouten, Kim and Frasincar, Flavius},
	month = mar,
	year = {2016},
	keywords = {Aspects, Data mining, Gain measurement, Joints, Linguistic processing, Loss measurement, Machine learning, Organizations, Sentiment analysis, Text analysis, Text mining, aspects, linguistic processing, machine learning, sentiment analysis, text analysis},
	pages = {813--830},
}

@inproceedings{Schouten2017OntologyOld,
	series = {{LNCS}},
	title = {Ontology-{Enhanced} {Aspect}-{Based} {Sentiment} {Analysis}},
	volume = {10360},
	abstract = {With many people freely expressing their opinions and feelings on the Web, much research has gone into modeling and monetizing opinionated, and usually unstructured and textual, Web-based content. Aspect-based sentiment analysis aims to extract the fine-grained topics, or aspects, that people are talking about, together with the sentiment expressed on those aspects. This allows for a detailed analysis of the sentiment expressed in, for instance, product and service reviews. In this work we focus on knowledge-driven solutions that aim to complement standard machine learning methods. By encoding common domain knowledge into a knowledge repository, or ontology, we are able to exploit this information to improve classification performance for both aspect detection and aspect sentiment analysis. For aspect detection, the ontology-enhanced method needs only 20\% of the training data to achieve results comparable with a standard bag-of-words approach that uses all training data.},
	language = {en},
	booktitle = {17th {International} {Conference} on {Web} {Engineering} ({ICWE} 2017)},
	publisher = {Springer},
	author = {Schouten, Kim and Frasincar, Flavius and de Jong, Franciska},
	year = {2017},
	pages = {302--320},
}

@inproceedings{Trusca2020HAABSA++Old,
	series = {{LNCS}},
	title = {A {Hybrid} {Approach} for {Aspect}-{Based} {Sentiment} {Analysis} {Using} {Deep} {Contextual} {Word} {Embeddings} and {Hierarchical} {Attention}},
	volume = {12128},
	abstract = {The Web has become the main platform where people express their opinions about entities of interest and their associated aspects. Aspect-Based Sentiment Analysis (ABSA) aims to automatically compute the sentiment towards these aspects from opinionated text. In this paper we extend the state-of-the-art Hybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) method in two directions. First we replace the non-contextual word embeddings with deep contextual word embeddings in order to better cope with the word semantics in a given text. Second, we use hierarchical attention by adding an extra attention layer to the HAABSA high-level representations in order to increase the method flexibility in modeling the input data. Using two standard datasets (SemEval 2015 and SemEval 2016) we show that the proposed extensions improve the accuracy of the built model for ABSA.},
	language = {en},
	booktitle = {20th {International} {Conference} on {Web} {Engineering} ({ICWE} 2020)},
	publisher = {Springer},
	author = {Truşcǎ, Maria Mihaela and Wassenberg, Daan and Frasincar, Flavius and Dekker, Rommert},
	year = {2020},
	pages = {365--380},
}

@inproceedings{Wallaart2019HAABSAOld,
	title = {A {Hybrid} {Approach} for {Aspect}-{Based} {Sentiment} {Analysis} {Using} a {Lexicalized} {Domain} {Ontology} and {Attentional} {Neural} {Models}},
	volume = {11503},
	isbn = {9783030213473},
	language = {en},
	urldate = {2022-03-06},
	booktitle = {16th {Extended} {Semantic} {Web} {Conference} ({ESWB} 2019)},
	publisher = {Springer},
	author = {Wallaart, Olaf and Frasincar, Flavius},
	year = {2019},
	pages = {363--378},
}

@article{Meskele2020ALDONAr,
	title = {{ALDONAr}: {A} hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalized domain ontology and a regularized neural attention model},
	volume = {57},
	issn = {0306-4573},
	shorttitle = {{ALDONAr}},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457319310222},
	doi = {10.1016/j.ipm.2020.102211},
	abstract = {Aspect-based sentiment analysis allows one to compute the sentiment for an aspect in a certain context. One problem in this analysis is that words possibly carry different sentiments for different aspects. Moreover, an aspect’s sentiment might be highly influenced by the domain-specific knowledge. In order to tackle these issues, in this paper, we propose a hybrid solution for sentence-level aspect-based sentiment analysis using A Lexicalized Domain Ontology and a Regularized Neural Attention model (ALDONAr). The bidirectional context attention mechanism is introduced to measure the influence of each word in a given sentence on an aspect’s sentiment value. The classification module is designed to handle the complex structure of a sentence. The manually created lexicalized domain ontology is integrated to utilize the field-specific knowledge. Compared to the existing ALDONA model, ALDONAr uses BERT word embeddings, regularization, the Adam optimizer, and different model initialization. Moreover, its classification module is enhanced with two 1D CNN layers providing superior results on standard datasets.},
	language = {en},
	number = {3},
	urldate = {2022-05-23},
	journal = {Information Processing \& Management},
	author = {Meškelė, Donatas and Frasincar, Flavius},
	month = may,
	year = {2020},
	keywords = {Aspect-based sentiment analysis, Bidirectional gated neural network, Hybrid model, Lexicalized domain ontology, Regularization},
	pages = {102211},
}

@article{Zhang2022Survey,
	title = {A {Survey} on {Aspect}-{Based} {Sentiment} {Analysis}: {Tasks}, {Methods}, and {Challenges}},
	shorttitle = {A {Survey} on {Aspect}-{Based} {Sentiment} {Analysis}},
	url = {http://arxiv.org/abs/2203.01054},
	abstract = {As an important fine-grained sentiment analysis problem, aspect-based sentiment analysis (ABSA), aiming to analyze and understand people's opinions at the aspect level, has been attracting considerable interest in the last decade. To handle ABSA in different scenarios, various tasks have been introduced for analyzing different sentiment elements and their relations, including the aspect term, aspect category, opinion term, and sentiment polarity. Unlike early ABSA works focusing on a single sentiment element, many compound ABSA tasks involving multiple elements have been studied in recent years for capturing more complete aspect-level sentiment information. However, a systematic review of various ABSA tasks and their corresponding solutions is still lacking, which we aim to fill in this survey. More specifically, we provide a new taxonomy for ABSA which organizes existing studies from the axes of concerned sentiment elements, with an emphasis on recent advances of compound ABSA tasks. From the perspective of solutions, we summarize the utilization of pre-trained language models for ABSA, which improved the performance of ABSA to a new stage. Besides, techniques for building more practical ABSA systems in cross-domain/lingual scenarios are discussed. Finally, we review some emerging topics and discuss some open challenges to outlook potential future directions of ABSA.},
	urldate = {2022-05-23},
	journal = {arXiv:2203.01054 [cs]},
	author = {Zhang, Wenxuan and Li, Xin and Deng, Yang and Bing, Lidong and Lam, Wai},
	month = mar,
	year = {2022},
	keywords = {Computer Science - Computation and Language},
}

@article{Kumar2021CASC,
	title = {{BERT} {Based} {Semi}-{Supervised} {Hybrid} {Approach} for {Aspect} and {Sentiment} {Classification}},
	volume = {53},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-021-10596-6},
	doi = {10.1007/s11063-021-10596-6},
	abstract = {Aspect-based sentiment analysis (ABSA) includes two sub-tasks namely, aspect extraction and aspect-level sentiment classification. Most existing works address these sub-tasks independently by applying a supervised learning approach using labeled data. However, obtaining such labeled sentences is difficult and extremely expensive. Hence, it is important to solve ABSA without taking a dependency on labeled sentences. In this work, we propose a three-step semi-supervised hybrid approach that jointly detects an aspect and its associated sentiment in a given review sentence. The first step of our approach takes a small set of seed words for each aspect and sentiment class to construct respective semantically coherent class vocabularies. The second step makes use of these constructed vocabularies along with POS tags to label a subset of sentences from the training corpus. As we adopt a semi-automated approach to label the data, this process may induce noise in the labels during the annotation. In the last step, we use such labeled sentences to build a noise-robust deep neural network for aspect and sentiment classification. We conduct experiments on two real data sets to verify the effectiveness of our model (https://github.com/Raghu150999/UnsupervisedABSA).},
	language = {en},
	number = {6},
	urldate = {2022-05-23},
	journal = {Neural Processing Letters},
	author = {Kumar, Avinash and Gupta, Pranjal and Balan, Raghunathan and Neti, Lalita Bhanu Murthy and Malapati, Aruna},
	month = dec,
	year = {2021},
	pages = {4207--4224},
}

@article{Karamanolakis2019SeedOld,
	title = {Leveraging {Just} a {Few} {Keywords} for {Fine}-{Grained} {Aspect} {Detection} {Through} {Weakly} {Supervised} {Co}-{Training}},
	abstract = {User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., price, quality, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, unsupervised topic models often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches do not effectively leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher); in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous weakly supervised approaches (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.},
	urldate = {2022-05-09},
	journal = {arXiv:1909.00415 [cs, stat]},
	author = {Karamanolakis, Giannis and Hsu, Daniel and Gravano, Luis},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{Huang2020JASenOld,
	address = {Online},
	title = {Weakly-{Supervised} {Aspect}-{Based} {Sentiment} {Analysis} via {Joint} {Aspect}-{Sentiment} {Topic} {Embedding}},
	abstract = {Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts. We propose to first learn {\textbackslash}textlesssentiment, aspect{\textbackslash}textgreater joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4\% and 5.1\% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.},
	urldate = {2022-05-11},
	booktitle = {2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Jiaxin and Meng, Yu and Guo, Fang and Ji, Heng and Han, Jiawei},
	month = nov,
	year = {2020},
	pages = {6989--6999},
}

@inproceedings{Hu2004RulesOld,
	title = {Mining and {Summarizing} {Customer} {Reviews}},
	language = {en},
	urldate = {2022-03-06},
	booktitle = {10th {ACM} {SIGKDD} international conference on {Knowledge} discovery and {Data} {Mining} ({KDD} 2004)},
	publisher = {ACM},
	author = {Hu, Minqing and Liu, Bing},
	year = {2004},
	pages = {168--177},
}

@article{Blei2003LDA,
	title = {Latent dirichlet allocation},
	volume = {3},
	issn = {1532-4435},
	url = {https://dl.acm.org/doi/10.5555/944919.944937},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	journal = {The Journal of Machine Learning Research},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	month = mar,
	year = {2003},
	pages = {993--1022},
}

@article{Fazly2009UnsupervisedTokenClass,
	title = {Unsupervised {Type} and {Token} {Identification} of {Idiomatic} {Expressions}},
	volume = {35},
	url = {https://aclanthology.org/J09-1005},
	doi = {10.1162/coli.08-010-R1-07-048},
	number = {1},
	urldate = {2022-05-23},
	journal = {Computational Linguistics},
	author = {Fazly, Afsaneh and Cook, Paul and Stevenson, Suzanne},
	month = mar,
	year = {2009},
	pages = {61--103},
}

@article{Cui2020UnsupervisedMRC,
	title = {Unsupervised {Explanation} {Generation} for {Machine} {Reading} {Comprehension}},
	url = {http://arxiv.org/abs/2011.06737},
	abstract = {With the blooming of various Pre-trained Language Models (PLMs), Machine Reading Comprehension (MRC) has embraced significant improvements on various benchmarks and even surpass human performances. However, the existing works only target on the accuracy of the final predictions and neglect the importance of the explanations for the prediction, which is a big obstacle when utilizing these models in real-life applications to convince humans. In this paper, we propose a self-explainable framework for the machine reading comprehension task. The main idea is that the proposed system tries to use less passage information and achieve similar results compared to the system that uses the whole passage, while the filtered passage will be used as explanations. We carried out experiments on three multiple-choice MRC datasets, and found that the proposed system could achieve consistent improvements over baseline systems. To evaluate the explainability, we compared our approach with the traditional attention mechanism in human evaluations and found that the proposed system has a notable advantage over the latter one.},
	urldate = {2022-05-13},
	journal = {arXiv:2011.06737 [cs]},
	author = {Cui, Yiming and Liu, Ting and Wang, Shijin and Hu, Guoping},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{Devlin2019BERTold,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-03-09},
	booktitle = {17th {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({NAACL}-{HLT} 2019)},
	publisher = {ACL},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	keywords = {Computer Science - Computation and Language},
	pages = {4171--4186},
}

@article{Honnibal2020Spacy,
	title = {{spaCy}: {Industrial}-strength natural language processing in python},
	abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
	language = {en},
	journal = {Zenodo, Honolulu, HI, USA},
	author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
	year = {2020},
}
